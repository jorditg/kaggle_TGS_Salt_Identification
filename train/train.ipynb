{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from dataprocess import AugmentColor, ToTensor, ISICDataset\n",
    "from loss import ISICLoss\n",
    "from model import LinkNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16  # 32 if you have 16 GB of VRAM\n",
    "batch_per_epoch = 10000 // batch # batch per epoch\n",
    "model_path = './models/resnet18_001.pth'  # Name for the model save\n",
    "load_model_path = None  # Load pretrain path\n",
    "encoder = 'resnet18'  # Encoder type: 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'\n",
    "final = 'sigmoid'  # Output layer type. 'softmax' or 'sigmoid'\n",
    "\n",
    "# Image augmentation parameters\n",
    "gamma = 0.15#0.35\n",
    "brightness = 1.0#2.0\n",
    "colors = 0.10#0.25\n",
    "\n",
    "# ISIC image normalization constants\n",
    "mean = [0.486, 0.336, 0.275]\n",
    "std = [0.299, 0.234, 0.209]\n",
    "\n",
    "dataset_dir = \"../input2/data_segmentation_augmented/\"\n",
    "train_dir = dataset_dir + \"train\"\n",
    "val_dir = dataset_dir + \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train imgs: 32\n",
      "Val imgs: 32\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    AugmentColor(gamma, brightness, colors),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_image_paths = sorted(glob.glob(train_dir + \"/img/*.jpeg\"))[0:32]\n",
    "train_target_paths = sorted(glob.glob(train_dir + \"/msk/*.jpeg\"))[0:32]\n",
    "train_dataset = ISICDataset(train_image_paths,\n",
    "                            train_target_paths,\n",
    "                            img_transform=train_transform,\n",
    "                            trg_transform=mask_transform)\n",
    "\n",
    "val_image_paths = sorted(glob.glob(val_dir + \"/img/*.jpg\"))[0:32]\n",
    "val_target_paths = sorted(glob.glob(val_dir + \"/msk/*.jpg\"))[0:32]\n",
    "val_dataset = ISICDataset(val_image_paths,\n",
    "                          val_target_paths,\n",
    "                          img_transform=val_transform,\n",
    "                          trg_transform=mask_transform)\n",
    "\n",
    "print(\"Train imgs:\", train_dataset.__len__())\n",
    "print(\"Val imgs:\", val_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_mask(img, msk):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.imshow(img.permute(1,2,0).cpu().numpy())\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.imshow(msk.permute(1,2,0).squeeze().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "N = train_dataset.__len__()\n",
    "img, msk = train_dataset.__getitem__(random.randint(0,N))\n",
    "print(img.size())\n",
    "plot_image_mask(img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_loss = ISICLoss().to(device)\n",
    "val_loss = ISICLoss().to(device)\n",
    "\n",
    "model = LinkNet(1, 3, encoder, final).to(device)\n",
    "if load_model_path is not None:\n",
    "    state = torch.load(load_model_path)\n",
    "    model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    c_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, trg in val_loader:\n",
    "            img = img.type(torch.cuda.FloatTensor)\n",
    "            trg = trg.type(torch.cuda.FloatTensor)\n",
    "            pred = model(img)\n",
    "            loss = val_loss(pred, trg)\n",
    "            c_loss += loss.item()\n",
    "        c_loss /= val_dataset.__len__()\n",
    "    return c_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    losses = []\n",
    "    best_loss = val()\n",
    "    print(\"Start val loss:\", best_loss)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        s_time = time.time()\n",
    "        for i, (img, trg) in enumerate(train_loader):\n",
    "            if i > batch_per_epoch:\n",
    "                break\n",
    "            # get the inputs\n",
    "            img = img.type(torch.cuda.FloatTensor)\n",
    "            trg = trg.type(torch.cuda.FloatTensor)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            pred = model(img) \n",
    "            loss = train_loss(pred, trg)\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= train_dataset.__len__()\n",
    "        val_s = val()\n",
    "        val_s_f = round((2-val_s*batch)/2, 5)  # LB score on val\n",
    "        print(\"Epoch:\", epoch+1, \"train loss:\", round(running_loss, 5),\n",
    "              \"val loss\", round(val_s, 5), \"val score:\", val_s_f,\n",
    "              \"time:\", round(time.time()-s_time, 2), \"s\")\n",
    "        if val_s < best_loss:\n",
    "            torch.save(model.state_dict(), model_path[:-4] + '_cpt_' \\\n",
    "                       + str(val_s_f) + model_path[-4:])\n",
    "            best_loss = val_s\n",
    "            print(\"Checkpoint saved\")\n",
    "        losses.append([running_loss, val])\n",
    "        running_loss = 0.0\n",
    "    # Save the train result\n",
    "    torch.save(model.state_dict(), model_path[:-4] + '_res_'\\\n",
    "               + str(val_s_f) + model_path[-4:])\n",
    "    #print(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bpe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-01689847daab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bpe' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "\n",
    "# Define the train protocol here\n",
    "lr = 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "train(epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "train(epochs=20, batch_per_epoch=bpe, optimizer=optimizer)\n",
    "lr = 1e-6\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "train(epochs=10, batch_per_epoch=bpe, optimizer=optimizer)\n",
    "\n",
    "\n",
    "# Save the final result\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
